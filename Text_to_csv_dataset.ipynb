{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_to_csv_dataset",
      "provenance": [],
      "authorship_tag": "ABX9TyM7+PD8v+Loe2h0cU+3jIvp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shahriar10k/RAG-with-NSU/blob/main/Text_to_csv_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Text Files to CSV**\n",
        "\n",
        "First upload the zip folder containing all the annotated texts in the colab  runtime folder. Then set your desired size and make the csv. Then download and get nuts."
      ],
      "metadata": {
        "id": "ImBkWWZz6hTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "#file_dir =  Place the directory inside \" \" of the zip folders uploaded(preferably in \"content/\") in colab run time.\n",
        "file_dir = \"\"\n",
        "\n",
        "#target_dir= The directory where we want to unzip. By default inside the content folder.\n",
        "target_dir =\"/content/\"\n",
        "\n",
        "# Format of archive file\n",
        "archive_format = \"zip\"\n",
        "\n",
        "shutil.unpack_archive(file_dir, target_dir, archive_format)\n"
      ],
      "metadata": {
        "id": "_Fdh-axh6f88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will read each text file and use filename as title."
      ],
      "metadata": {
        "id": "fzHAfvmb8ekn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib\n",
        "\n",
        "titles = []\n",
        "articles = []\n",
        "\n",
        "print('Reading in all articles.....')\n",
        "\n",
        "i=0\n",
        "\n",
        "#fold_dir= Place the folder directory of the unzipped text files. Must add a '/'. Ex- \"/content/499Dataset/\"\n",
        "fold_dir=\"/content/499Dataset/\"\n",
        "\n",
        "#Read for each of the files in the subdirectory....\n",
        "for filename in os.listdir(fold_dir):\n",
        "\n",
        "    #Skip non text files\n",
        "    if not filename[-3:] == \"txt\":\n",
        "      continue\n",
        "\n",
        "    #Open the file\n",
        "    with open(fold_dir+ filename, \"rb\") as f:\n",
        "        # Use the file name as the article title (strip the \".txt\")\n",
        "         titles.append(filename[:-4])\n",
        "\n",
        "         # Read in the article text.\n",
        "         articles.append(f.read().decode('latin-1'))\n",
        "\n",
        "print(' Done.\\n')\n",
        "\n",
        "print(' There are {:,} articles.' .format(len(articles)))"
      ],
      "metadata": {
        "id": "Jp8Berhu8Xha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will split the text files content for our passage size. These passage size can be 100,200 or anything and then this will be tokenized once more by BERT tokenizer.  \n",
        "\n",
        "**Use slice_limit variable to set passage length**\n"
      ],
      "metadata": {
        "id": "B_1XpSgP98AG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Before splitting, {:,} articles.\\n'.format(len(titles)))\n",
        "\n",
        "#Two lists : titles and articles\n",
        "\n",
        "passage_titles = []\n",
        "passages = []\n",
        "\n",
        "#Enter the slicing limit.Ex -100,200,300\n",
        "slice_limit = 100\n",
        "print(\"Splitting.........\")\n",
        "\n",
        "#For each article and its title....\n",
        "\n",
        "for i in range(len(titles)):\n",
        "\n",
        "    title = titles[i]\n",
        "    article = articles[i]\n",
        "\n",
        "    #Skip if there is no content or empty\n",
        "    if len(articles) == 0:\n",
        "      print(\"Skipping empty article:\", title)\n",
        "      continue\n",
        "\n",
        "    #Split the text on white space.\n",
        "    #By default, this will remove all whitespace, including newline tab and \n",
        "    #characters\n",
        "    words = article.split()\n",
        "    #print(\"words: \\n\")\n",
        "    #print(words)\n",
        "    #print(\"\\n\")\n",
        "\n",
        "    #Loop over the words, incrementing by slice_limit.\n",
        "    for i in range(0, len(words),slice_limit):\n",
        "\n",
        "      #Select the next 100 words\n",
        "      #Python slices automatically stop at the end of the array.\n",
        "      chunk_words = words[i : i + slice_limit]\n",
        "      #print(\"Chunk words: \\n\")\n",
        "      #print(chunk_words)\n",
        "      #print(\"\\n\")\n",
        "\n",
        "      #Recombine the words into a passage by joining with whitespace\n",
        "      chunk = \" \".join(chunk_words)\n",
        "\n",
        "      #Remove any trailing whitespace.\n",
        "      chunk = chunk.strip()\n",
        "\n",
        "      #To avoid a possible edge case, skip any empty chunks.\n",
        "      if len(chunk) == 0:\n",
        "        continue\n",
        "      \n",
        "      #Store the chunk. Every chunk in the article uses the article title\n",
        "    \n",
        "      #print(chunk)\n",
        "      passage_titles.append(title)\n",
        "      passages.append(chunk)\n",
        "\n",
        "print(' Done.\\n')\n",
        "\n",
        "chunked_corpus = {'title': passage_titles, 'text' : passages}\n",
        "\n",
        "print('After splitting, {:,} \"passages\".'.format(len(chunked_corpus['title'])))"
      ],
      "metadata": {
        "id": "dYl5FeM--Mtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the csv file. Download the file from colab runtime folder to your local device"
      ],
      "metadata": {
        "id": "HNNdSnps-60z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(chunked_corpus)\n",
        "\n",
        "#Write it out to disk. Comma(,) or tab(\\t) separator can be used \n",
        "df.to_csv(r\"/content/curated_dataset_\"+str(slice_limit)+\".csv\" ,sep=',', index=False)"
      ],
      "metadata": {
        "id": "rYoLhgQJ-3pp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}